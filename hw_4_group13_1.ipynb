{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metaparametrs\n",
    "epochs = 5 # more epochs lead to increased accuracy\n",
    "batch_size = 200 #CNN learns better with this butch size\n",
    "validation_batch_size = 500\n",
    "learning_rate = 0.0001\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svhn(path):\n",
    "    \n",
    "    def normalize(array):\n",
    "        mean = np.mean(array)\n",
    "        st = np.std(array)\n",
    "        array = (array-mean)/st\n",
    "        return array\n",
    "    \n",
    "    matfile = scipy.io.loadmat(path)\n",
    "    images = matfile['X']\n",
    "    images = normalize(images)\n",
    "    images = np.expand_dims(images,0)\n",
    "    images = np.swapaxes(images, 0,4)\n",
    "    images = np.squeeze(images)\n",
    "    labels = matfile['y']\n",
    "    labels = np.squeeze(labels)\n",
    "    \n",
    "    validation_images = images[:1000,:,:,:]\n",
    "    training_images = images[1000:5000,:,:,:]\n",
    "    validation_labels = labels[:1000]\n",
    "    training_labels = labels[1000:5000]\n",
    "    \n",
    "    return training_images, training_labels, validation_images, validation_labels\n",
    "\n",
    "\n",
    "training_data, training_labels, validation_data, validation_labels = load_svhn('../train_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We want to investigate the normilized data. To see actual names of category we set title to \"label_to_word\"\n",
    "# We can see that images differ from original ones. \n",
    "fig, ax = plt.subplots(1,10,figsize=(10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i].imshow(training_data[i])\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(training_labels[i]) # we take i_th training label and use it as a key of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Getting data\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_data, training_labels))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_data, validation_labels))\n",
    "\n",
    "# This allows us to directly define batchsize we want to use.\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "validation_dataset = validation_dataset.batch(validation_batch_size)\n",
    "\n",
    "# Shuffle the training data in each epoch.\n",
    "training_dataset = training_dataset.shuffle(buffer_size=4, reshuffle_each_iteration=True)\n",
    "# Shuffle the training data in each epoch.\n",
    "#training_dataset = training_dataset.shuffle(buffer_size=4, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need an iterator which will allow us to iterate throught the dataset.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# And we prepare initializer\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input for Network\n",
    "\n",
    "input_data  = next_batch[0]\n",
    "input_data = tf.cast(input_data, tf.float32)\n",
    "\n",
    "labels = next_batch[1]\n",
    "labels = tf.cast(labels, tf.int64)\n",
    "labels_one_hot = tf.one_hot(labels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to use dropout technique which allows to redce overfitting problem. \n",
    "#We will dumpen 50% of neurons during forward step, so not all of them will learn on each batch. BUT! We do not want to\n",
    "#use dropout during validation. So we create a placeholder, which we will feed during training\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope(\"convolution_1\"):\n",
    "    kernels = tf.Variable(tf.truncated_normal([5,5,3,16],stddev = 0.1))\n",
    "    biases  = tf.Variable(tf.zeros([16]))\n",
    "    convolution = tf.nn.conv2d(input_data, kernels,strides = [1, 1, 1, 1],padding = \"SAME\")\n",
    "    feature_maps = tf.nn.relu(convolution + biases)\n",
    "    \n",
    "    tf.summary.histogram(\"convolution_1_weights\", kernels)\n",
    "    tf.summary.histogram(\"convolution_1_biases\", biases)\n",
    "\n",
    "with tf.variable_scope(\"pooling_1\"):\n",
    "    pooling = tf.nn.max_pool(feature_maps,\n",
    "                             ksize = [1, 2, 2, 1],\n",
    "                             strides = [1, 2, 2, 1],\n",
    "                             padding = \"SAME\")\n",
    "\n",
    "with tf.variable_scope(\"convolution_2\"):\n",
    "    kernels = tf.Variable(tf.truncated_normal([3,3,16,32], stddev = 0.1))\n",
    "    biases  = tf.Variable(tf.zeros([32]))\n",
    "    convolution = tf.nn.conv2d(pooling,kernels,strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "    feature_maps = tf.nn.relu(convolution + biases)\n",
    "    \n",
    "    tf.summary.histogram(\"convolution_2_weights\", kernels)\n",
    "    tf.summary.histogram(\"convolution_2_biases\", biases)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"pooling_2\"):\n",
    "    pooling = tf.nn.max_pool(feature_maps,\n",
    "                             ksize = [1, 2, 2, 1],\n",
    "                             strides = [1, 2, 2, 1],\n",
    "                             padding = \"SAME\")\n",
    "    flat = tf.reshape(pooling, [-1, 8*8*32])\n",
    "\n",
    "    \n",
    "with tf.variable_scope(\"fully_connected\"):\n",
    "    weights = tf.Variable(tf.truncated_normal([2*8*128, 512],stddev=2e-06))\n",
    "    bias = tf.Variable(tf.constant(0.0, shape = [512]))\n",
    "\n",
    "    pre_act = tf.add(tf.matmul(flat, weights), bias)\n",
    "    activation = tf.nn.tanh(pre_act)\n",
    "    #activation = tf.nn.dropout(activation, keep_prob = keep_prob)\n",
    "    \n",
    "    tf.summary.histogram(\"fully_connected_weights\", weights)\n",
    "    tf.summary.histogram(\"fully_connected1_biases\", bias)\n",
    "    \n",
    "        \n",
    "with tf.variable_scope(\"read_out\"):\n",
    "    weights = tf.get_variable(\"weights\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bias = tf.Variable(tf.constant(0.0, shape = [10]))\n",
    "    logits = tf.add(tf.matmul(activation, weights), bias)\n",
    "    \n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    tf.summary.histogram(\"read_out\", weights)\n",
    "    tf.summary.histogram(\"read_out_biases\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels_one_hot)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Accuracy.\n",
    "accuracy = tf.equal(tf.argmax(output, 1), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Than we define an Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# We tell it to minimize loss for each training step\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all summaries and write in two folders\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('./summaries/train/')\n",
    "validation_writer = tf.summary.FileWriter('./summaries/validation/', flush_secs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time() # we want to monitor how much time does it take a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 5\n",
      "1.092057466506958\n",
      "ACCURACY: 0.2\n",
      "ACCURACY: 0.245\n",
      "ACCURACY: 0.17\n",
      "ACCURACY: 0.24\n",
      "ACCURACY: 0.215\n",
      "ACCURACY: 0.105\n",
      "ACCURACY: 0.12\n",
      "ACCURACY: 0.12\n",
      "ACCURACY: 0.185\n",
      "ACCURACY: 0.23\n",
      "Validation accuracy: 0.198\n",
      "Epoch: 2 / 5\n",
      "11.091220617294312\n",
      "ACCURACY: 0.22\n",
      "ACCURACY: 0.19\n",
      "ACCURACY: 0.17\n",
      "ACCURACY: 0.215\n",
      "ACCURACY: 0.16\n",
      "ACCURACY: 0.19\n",
      "ACCURACY: 0.26\n",
      "ACCURACY: 0.245\n",
      "ACCURACY: 0.27\n",
      "ACCURACY: 0.24\n",
      "Validation accuracy: 0.236\n",
      "Epoch: 3 / 5\n",
      "16.802045345306396\n",
      "ACCURACY: 0.33\n",
      "ACCURACY: 0.32\n",
      "ACCURACY: 0.255\n",
      "ACCURACY: 0.28\n",
      "ACCURACY: 0.195\n",
      "ACCURACY: 0.2\n",
      "ACCURACY: 0.245\n",
      "ACCURACY: 0.24\n",
      "ACCURACY: 0.245\n",
      "ACCURACY: 0.25\n",
      "Validation accuracy: 0.262\n",
      "Epoch: 4 / 5\n",
      "22.221888065338135\n",
      "ACCURACY: 0.325\n",
      "ACCURACY: 0.245\n",
      "ACCURACY: 0.255\n",
      "ACCURACY: 0.3\n",
      "ACCURACY: 0.25\n",
      "ACCURACY: 0.37\n",
      "ACCURACY: 0.27\n",
      "ACCURACY: 0.3\n",
      "ACCURACY: 0.335\n",
      "ACCURACY: 0.265\n",
      "Validation accuracy: 0.31\n",
      "Epoch: 5 / 5\n",
      "27.010569095611572\n",
      "ACCURACY: 0.345\n",
      "ACCURACY: 0.375\n",
      "ACCURACY: 0.405\n",
      "ACCURACY: 0.365\n",
      "ACCURACY: 0.32\n",
      "ACCURACY: 0.255\n",
      "ACCURACY: 0.325\n",
      "ACCURACY: 0.345\n",
      "ACCURACY: 0.325\n",
      "ACCURACY: 0.29\n",
      "Validation accuracy: 0.284\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Let's train our model.\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # First initialize the variables.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # We need a step counter.\n",
    "    global_step = 0\n",
    "    \n",
    "    # Now we set up a loop that will run until we finished all epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch+1, \"/\",epochs)\n",
    "        print(time.time() - start_t)    \n",
    "        # TRAINING\n",
    "        sess.run(training_init_op)\n",
    "         \n",
    "        # In each epoch we want to go through all batches of the training data. \n",
    "        while True:\n",
    "            try:\n",
    "                # Instead of the value we read out the summary and we feed our keep_prob placeholder with 0.5\n",
    "                # so 50% of neurons during training are dumped. But we do not \n",
    "                _, summary  = sess.run((training_step, merged_summaries), feed_dict={keep_prob: 1.0})\n",
    "                loss_value, accuracy_value = sess.run((loss, accuracy))\n",
    "                print('ACCURACY:', accuracy_value)\n",
    "                \n",
    "                # We write the summary into our folder.\n",
    "                train_writer.add_summary(summary, global_step)\n",
    "                #print(3, time.time())\n",
    "                # And update the step counter.\n",
    "                global_step += 1\n",
    "            \n",
    "            # Breakout of the loop if we looked at all batches\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        # VALIDATION\n",
    "        epoch+=1\n",
    "        sess.run(validation_init_op)\n",
    "        summary = sess.run((merged_summaries), feed_dict ={keep_prob: 1.0}) #keep_prob = 1, so all neurons active\n",
    "        validation_writer.add_summary(summary, global_step)\n",
    "        \n",
    "        loss_value, acc_val = sess.run((loss, accuracy), feed_dict ={keep_prob: 1.0})\n",
    "        print(\"Validation accuracy:\", acc_val)\n",
    "        \n",
    "print(\"Finished\")\n",
    "\n",
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "\n",
    "sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
